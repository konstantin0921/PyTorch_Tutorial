{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('pytorch_env': conda)",
   "display_name": "Python 3.7.9 64-bit ('pytorch_env': conda)",
   "metadata": {
    "interpreter": {
     "hash": "4186f74e01e939e2abb13312658e7bf0ac5f4e211937d1494f0d00c60e38bb4e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Sequence Models\n",
    "\n",
    "In this tutorial, you will learn about LSTM neural networks and see an example of how they can be used to recognize parts of speech."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12557b650>"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "from IPython.display import Image\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "source": [
    "## Introduction\n",
    "\n",
    "### RNNs\n",
    "* Difficulty learning widely separated relationships in long sequences. ### What is an LSTM?\n",
    "* Long Short-Term Memory.\n",
    "* Able to learn long term dependencies in a sequence.\n",
    "* Uses a cell state and gates to remember relationships.\n",
    "\n",
    "**Example**: *The humidity is very high. Today it is going to **rain**. A regular RNN might not do a good job predicting the word **rain**.*\n",
    "\n",
    "### Rolled Out LSTM Network\n",
    "\n",
    "![rolled out lstm](data/roll_out.png)\n",
    "\n",
    "### LSTM Cell\n",
    "\n",
    "![lstm cell](data/lstm_cell.png)\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Input Gate.\n",
    "* Decide what information to store in cell state.\n",
    "* Decide what to update.\n",
    "  * Sigmoid layer.\n",
    "* Create candidate values for cell state.\n",
    "  *  Tanh layer.\n",
    "\n",
    "### Example Continued:\n",
    "\n",
    "* replace the previous rain likelihood with the new likelihood for dry weather.\n",
    "\n",
    "![](data/LSTM3-focus-i.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Update Cell State\n",
    "* Remove forgotten information.\n",
    "  * Multiply $f_t$ by old cell state.\n",
    "* Add new candidate values scaled by their importance.\n",
    "\n",
    "  * Add $i_t\\ast\\tilde{C}_t$\n",
    "\n",
    "  ![](data/LSTM3-focus-C.png)\n",
    "\n",
    "### Output Gate\n",
    "* Return filtered version of the cell state.\n",
    "\n",
    "### Example Continued:\n",
    "\n",
    "* Keep track of humidity magnitude to help the network decide whether to predict a large storm or just light rain.\n",
    "\n",
    "![](data/LSTM3-focus-o.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Very Simple Example 1\n",
    "Create an LSTM in PyTorch using a for loop."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x_1: tensor([[[0.1286]]], grad_fn=<StackBackward>)\nh_1: (tensor([[[0.1286]]], grad_fn=<StackBackward>), tensor([[[0.5273]]], grad_fn=<StackBackward>))\n\nx_2: tensor([[[0.1395]]], grad_fn=<StackBackward>)\nh_2: (tensor([[[0.1395]]], grad_fn=<StackBackward>), tensor([[[0.4831]]], grad_fn=<StackBackward>))\n\nx_3: tensor([[[0.1322]]], grad_fn=<StackBackward>)\nh_3: (tensor([[[0.1322]]], grad_fn=<StackBackward>), tensor([[[0.5156]]], grad_fn=<StackBackward>))\n\nx_4: tensor([[[0.1449]]], grad_fn=<StackBackward>)\nh_4: (tensor([[[0.1449]]], grad_fn=<StackBackward>), tensor([[[0.4218]]], grad_fn=<StackBackward>))\n\nx_5: tensor([[[0.1443]]], grad_fn=<StackBackward>)\nh_5: (tensor([[[0.1443]]], grad_fn=<StackBackward>), tensor([[[0.4398]]], grad_fn=<StackBackward>))\n\n"
     ]
    }
   ],
   "source": [
    "# define LSTM \n",
    "SEQUENCE_LEN = 5  # The length of the sequence\n",
    "INPUT_SIZE = 1  # Number of input features per time step\n",
    "HIDDEN_SIZE = 1  # Number of LSTM blocks per layer of the RNN\n",
    "BATCH_SIZE = 1  # The batch size\n",
    "\n",
    "lstm = nn.LSTM(input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE)\n",
    "\n",
    "# create fake inputs for LSTM\n",
    "inputs = torch.randn(SEQUENCE_LEN, BATCH_SIZE, INPUT_SIZE)\n",
    "\n",
    "# initialize the hidden state and cell states\n",
    "hidden_0 = torch.randn(1, BATCH_SIZE, HIDDEN_SIZE)\n",
    "cell_0 = torch.randn(1, BATCH_SIZE, HIDDEN_SIZE)\n",
    "\n",
    "# Step through the LSTM as it takes in the input sequence\n",
    "for i, in_value in enumerate(inputs):\n",
    "    # Step through the sequence one element at a time.\n",
    "    # After each time step, hidden contains the hidden state.\n",
    "    out, hidden_out = lstm(in_value.view(1, 1, -1), (hidden_0, cell_0))\n",
    "    print('x_{}: {}'.format(i+1, out))\n",
    "    print('h_{}: {}'.format(i+1, hidden_out))\n",
    "    print('')"
   ]
  },
  {
   "source": [
    "Here, x is the output and h is the value of the hidden and cell states at each step in the sequence\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Very Simple Example 2\n",
    "Create an LSTM in PyTorch using cat."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "out: tensor([[[-0.1616]],\n\n        [[-0.1378]],\n\n        [[ 0.0745]],\n\n        [[-0.2183]],\n\n        [[-0.2227]]], grad_fn=<StackBackward>)\nlast hidden and cell states: (tensor([[[-0.2227]]], grad_fn=<StackBackward>), tensor([[[-0.9171]]], grad_fn=<StackBackward>))\n"
     ]
    }
   ],
   "source": [
    "# Define LSTM architecture\n",
    "SEQUENCE_LEN = 5  # The length of the sequence\n",
    "INPUT_SIZE = 1  # Number of input features per time step\n",
    "HIDDEN_SIZE = 1  # Number of LSTM blocks per layer of the RNN\n",
    "BATCH_SIZE = 1  # The batch size\n",
    "lstm = nn.LSTM(input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE)\n",
    "\n",
    "# create the inputs for the LSTM\n",
    "inputs = [torch.randn(BATCH_SIZE, INPUT_SIZE) for _ in range(SEQUENCE_LEN)]\n",
    "\n",
    "# Concatenate the inputs so that they are a tensor\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "\n",
    "# initialize the hidden and cell states\n",
    "hidden_0 = torch.randn(1, BATCH_SIZE, HIDDEN_SIZE)\n",
    "cell_0 = torch.randn(1, BATCH_SIZE, HIDDEN_SIZE)\n",
    "\n",
    "# out = all states, hidden = last state and last cell state\n",
    "out, hidden = lstm(inputs, (hidden_0, cell_0))\n",
    "\n",
    "\n",
    "print('out: {}'.format(out))\n",
    "print('last hidden and cell states: {}'.format(hidden))"
   ]
  },
  {
   "source": [
    "## Example: An LSTM for Part-of-Speech Tagging\n",
    "* Predict parts-of-speach in a sentence.\n",
    "\n",
    "### Prepare the data:\n",
    "Training data is a list of list pairs.\n",
    "* First list is a sentence.\n",
    "* Second list are the parts-of-speech tags for each word in the sentence."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [\n",
    "    (\"The dog ate the apple.\".split(), [\"Determiner\", \"Noun\", \"Verb\", \"Determiner\", \"Noun\"]),\n",
    "    (\"Everybody read that book.\".split(), [\"Noun\", \"Verb\", \"Determiner\", \"Noun\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['The', 'dog', 'ate', 'the', 'apple.'],\n",
       " ['Everybody', 'read', 'that', 'book.']]"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "training_sentences = [sentence for sentence, tags in training_data]\n",
    "training_sentences"
   ]
  },
  {
   "source": [
    "### Clean the data\n",
    "Make all words lower case and remove punctuation.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_words(words_list):\n",
    "    '''lowercase words and remove punctuation\n",
    "    returns: clean words list\n",
    "    '''\n",
    "    return [word.lower().split('.')[0] for word in words_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(['the', 'dog', 'ate', 'the', 'apple'],\n",
       "  ['Determiner', 'Noun', 'Verb', 'Determiner', 'Noun']),\n",
       " (['everybody', 'read', 'that', 'book'],\n",
       "  ['Noun', 'Verb', 'Determiner', 'Noun'])]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "training_data_clean = [(clean_words(sentence), tags) for sentence, tags in training_data]\n",
    "training_data_clean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['the', 'dog', 'ate', 'the', 'apple'], ['everybody', 'read', 'that', 'book']]"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "training_sentences_clean = [sentence for sentence, tag in training_data_clean]\n",
    "training_sentences_clean"
   ]
  },
  {
   "source": [
    "### Create vocabulary\n",
    "Using all words in each sentence of the training data, create a vocabulary."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['apple', 'read', 'ate', 'that', 'the', 'dog', 'everybody', 'book']\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "\n",
    "for sentence in training_sentences_clean:\n",
    "    words += sentence\n",
    "\n",
    "vocab = list(set(words))\n",
    "print(vocab)"
   ]
  },
  {
   "source": [
    "### Create mapping dictionaries\n",
    "Using dictionaries to convert words to integers."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'apple': 0, 'read': 1, 'ate': 2, 'that': 3, 'the': 4, 'dog': 5, 'everybody': 6, 'book': 7}\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {word:i for i, word in enumerate(vocab)}\n",
    "print(word_to_ix)"
   ]
  },
  {
   "source": [
    "Map the parts-of-speech tags to integers:\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tags to integers\n",
    "tag_to_ix = dict([('Determiner', 0), ('Noun', 1), ('Verb',2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'Determiner': 0, 'Noun': 1, 'Verb': 2}"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "tag_to_ix"
   ]
  },
  {
   "source": [
    "Map the integers back to parts-of-speech.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{0: 'Determiner', 1: 'Noun', 2: 'Verb'}"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "ix_to_tag = {i:tag for tag, i in tag_to_ix.items()}\n",
    "ix_to_tag"
   ]
  },
  {
   "source": [
    "## Set Hyperparameters\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6\n",
    "LEARNING_RATE = 0.1\n",
    "NUM_EPOCHS = 300"
   ]
  },
  {
   "source": [
    "## Create the model\n",
    "\n",
    "LSTMTagger class.\n",
    "\n",
    "* Inherits nn.Module from PyTorch.\n",
    "* Inputs:\n",
    "    * Embedding dimension.\n",
    "    *  Number of hidden dimensions.\n",
    "    *  Vocabulary size.\n",
    "    *  Tag set size.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # LSTM: Inputs are embeddings, outputs are hidden states\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        # Linear layer maps hidden space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        #Initialize the hidden state. The axes correspond to (num_layers, minibatch_size, hidden_dim).\n",
    "        return (torch.zeros(1, 1, self.hidden_dim),\n",
    "                torch.zeros(1, 1, self.hidden_dim))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        '''\n",
    "        Make a forward pass through the LSTM\n",
    "\n",
    "        :param sentence: The input sentence\n",
    "        :type sentence: list\n",
    "        :return: A Tensor of tag scores.\n",
    "        :rtype:: Tensor\n",
    "        '''\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Helper Function\n",
    "Map either words or tags to integers, using the previously defined dictionaries (tag_to_ix, ix_to_tag)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    '''\n",
    "    Convert words or tags to integers and return a Tensor\n",
    "    :param seq: sequence of words\n",
    "    :type seq: list\n",
    "    :param to_ix: Dictionary mapping words or tags to integers\n",
    "    :return: A Pytorch tensor of indices\n",
    "    :rtype: Tensor\n",
    "    '''\n",
    "    idx = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idx, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([4])"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "# test\n",
    "prepare_sequence(['the'], word_to_ix)"
   ]
  },
  {
   "source": [
    "## Train the model:\n",
    "Create the LSTM Pytorch model using the hyperparameters defined above."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(vocab), len(tag_to_ix))"
   ]
  },
  {
   "source": [
    "Define the loss function. In this case, we will be using a negative log likelihood function, which is useful in classification problems.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.NLLLoss()"
   ]
  },
  {
   "source": [
    "### Negative Log Likelihood\n",
    "We can illustrate negative log likelihood in the following diagram:\n",
    "\n",
    "![data/nll.png](data/nll.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We will train the model using stochastic gradient descent.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "source": [
    "Let's run the model before any training has been done and store the scores to a **list**. We will then compare these scores with the scores after training.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['The', 'dog', 'ate', 'the', 'apple.'],\n",
       " ['Everybody', 'read', 'that', 'book.']]"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "training_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_initial_probabilities = []\n",
    "store_initial_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sentence in training_sentences_clean:\n",
    "        inputs = prepare_sequence(sentence, word_to_ix)\n",
    "        tag_scores = model(inputs)\n",
    "        tag_probabilities = tag_scores.exp()\n",
    "        # print(tag_probabilities)\n",
    "        max_values, max_indices = torch.max(tag_probabilities, dim=1)\n",
    "        initial_prediction = [ix_to_tag[x] for x in max_indices.numpy()]\n",
    "        store_initial_predictions.append(initial_prediction)\n",
    "        store_initial_probabilities.append(tag_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[tensor([[0.2398, 0.2627, 0.4975],\n        [0.2419, 0.2779, 0.4802],\n        [0.2534, 0.2684, 0.4782],\n        [0.2489, 0.2651, 0.4860],\n        [0.2222, 0.2934, 0.4845]]), tensor([[0.2205, 0.3022, 0.4773],\n        [0.2305, 0.2911, 0.4784],\n        [0.2230, 0.2964, 0.4807],\n        [0.2240, 0.2953, 0.4807]])]\n"
     ]
    }
   ],
   "source": [
    "print(store_initial_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['Verb', 'Verb', 'Verb', 'Verb', 'Verb'], ['Verb', 'Verb', 'Verb', 'Verb']]\n"
     ]
    }
   ],
   "source": [
    "print(store_initial_predictions)"
   ]
  },
  {
   "source": [
    "Now, we will train the model.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(['the', 'dog', 'ate', 'the', 'apple'],\n",
       "  ['Determiner', 'Noun', 'Verb', 'Determiner', 'Noun']),\n",
       " (['everybody', 'read', 'that', 'book'],\n",
       "  ['Noun', 'Verb', 'Determiner', 'Noun'])]"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "training_data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 1, loss: 0.0402\n",
      "epoch: 1, loss: 0.0526\n",
      "epoch: 2, loss: 0.0397\n",
      "epoch: 2, loss: 0.0521\n",
      "epoch: 3, loss: 0.0393\n",
      "epoch: 3, loss: 0.0515\n",
      "epoch: 4, loss: 0.0388\n",
      "epoch: 4, loss: 0.0510\n",
      "epoch: 5, loss: 0.0384\n",
      "epoch: 5, loss: 0.0505\n",
      "epoch: 6, loss: 0.0380\n",
      "epoch: 6, loss: 0.0500\n",
      "epoch: 7, loss: 0.0376\n",
      "epoch: 7, loss: 0.0495\n",
      "epoch: 8, loss: 0.0371\n",
      "epoch: 8, loss: 0.0491\n",
      "epoch: 9, loss: 0.0367\n",
      "epoch: 9, loss: 0.0486\n",
      "epoch: 10, loss: 0.0364\n",
      "epoch: 10, loss: 0.0481\n",
      "epoch: 11, loss: 0.0360\n",
      "epoch: 11, loss: 0.0477\n",
      "epoch: 12, loss: 0.0356\n",
      "epoch: 12, loss: 0.0472\n",
      "epoch: 13, loss: 0.0352\n",
      "epoch: 13, loss: 0.0468\n",
      "epoch: 14, loss: 0.0349\n",
      "epoch: 14, loss: 0.0464\n",
      "epoch: 15, loss: 0.0345\n",
      "epoch: 15, loss: 0.0459\n",
      "epoch: 16, loss: 0.0342\n",
      "epoch: 16, loss: 0.0455\n",
      "epoch: 17, loss: 0.0338\n",
      "epoch: 17, loss: 0.0451\n",
      "epoch: 18, loss: 0.0335\n",
      "epoch: 18, loss: 0.0447\n",
      "epoch: 19, loss: 0.0332\n",
      "epoch: 19, loss: 0.0443\n",
      "epoch: 20, loss: 0.0328\n",
      "epoch: 20, loss: 0.0439\n",
      "epoch: 21, loss: 0.0325\n",
      "epoch: 21, loss: 0.0435\n",
      "epoch: 22, loss: 0.0322\n",
      "epoch: 22, loss: 0.0432\n",
      "epoch: 23, loss: 0.0319\n",
      "epoch: 23, loss: 0.0428\n",
      "epoch: 24, loss: 0.0316\n",
      "epoch: 24, loss: 0.0424\n",
      "epoch: 25, loss: 0.0313\n",
      "epoch: 25, loss: 0.0421\n",
      "epoch: 26, loss: 0.0310\n",
      "epoch: 26, loss: 0.0417\n",
      "epoch: 27, loss: 0.0307\n",
      "epoch: 27, loss: 0.0414\n",
      "epoch: 28, loss: 0.0304\n",
      "epoch: 28, loss: 0.0410\n",
      "epoch: 29, loss: 0.0302\n",
      "epoch: 29, loss: 0.0407\n",
      "epoch: 30, loss: 0.0299\n",
      "epoch: 30, loss: 0.0403\n",
      "epoch: 31, loss: 0.0296\n",
      "epoch: 31, loss: 0.0400\n",
      "epoch: 32, loss: 0.0294\n",
      "epoch: 32, loss: 0.0397\n",
      "epoch: 33, loss: 0.0291\n",
      "epoch: 33, loss: 0.0394\n",
      "epoch: 34, loss: 0.0288\n",
      "epoch: 34, loss: 0.0391\n",
      "epoch: 35, loss: 0.0286\n",
      "epoch: 35, loss: 0.0388\n",
      "epoch: 36, loss: 0.0283\n",
      "epoch: 36, loss: 0.0384\n",
      "epoch: 37, loss: 0.0281\n",
      "epoch: 37, loss: 0.0381\n",
      "epoch: 38, loss: 0.0279\n",
      "epoch: 38, loss: 0.0378\n",
      "epoch: 39, loss: 0.0276\n",
      "epoch: 39, loss: 0.0376\n",
      "epoch: 40, loss: 0.0274\n",
      "epoch: 40, loss: 0.0373\n",
      "epoch: 41, loss: 0.0272\n",
      "epoch: 41, loss: 0.0370\n",
      "epoch: 42, loss: 0.0269\n",
      "epoch: 42, loss: 0.0367\n",
      "epoch: 43, loss: 0.0267\n",
      "epoch: 43, loss: 0.0364\n",
      "epoch: 44, loss: 0.0265\n",
      "epoch: 44, loss: 0.0362\n",
      "epoch: 45, loss: 0.0263\n",
      "epoch: 45, loss: 0.0359\n",
      "epoch: 46, loss: 0.0261\n",
      "epoch: 46, loss: 0.0356\n",
      "epoch: 47, loss: 0.0259\n",
      "epoch: 47, loss: 0.0354\n",
      "epoch: 48, loss: 0.0257\n",
      "epoch: 48, loss: 0.0351\n",
      "epoch: 49, loss: 0.0255\n",
      "epoch: 49, loss: 0.0348\n",
      "epoch: 50, loss: 0.0253\n",
      "epoch: 50, loss: 0.0346\n",
      "epoch: 51, loss: 0.0251\n",
      "epoch: 51, loss: 0.0343\n",
      "epoch: 52, loss: 0.0249\n",
      "epoch: 52, loss: 0.0341\n",
      "epoch: 53, loss: 0.0247\n",
      "epoch: 53, loss: 0.0339\n",
      "epoch: 54, loss: 0.0245\n",
      "epoch: 54, loss: 0.0336\n",
      "epoch: 55, loss: 0.0243\n",
      "epoch: 55, loss: 0.0334\n",
      "epoch: 56, loss: 0.0241\n",
      "epoch: 56, loss: 0.0332\n",
      "epoch: 57, loss: 0.0239\n",
      "epoch: 57, loss: 0.0329\n",
      "epoch: 58, loss: 0.0238\n",
      "epoch: 58, loss: 0.0327\n",
      "epoch: 59, loss: 0.0236\n",
      "epoch: 59, loss: 0.0325\n",
      "epoch: 60, loss: 0.0234\n",
      "epoch: 60, loss: 0.0323\n",
      "epoch: 61, loss: 0.0232\n",
      "epoch: 61, loss: 0.0320\n",
      "epoch: 62, loss: 0.0231\n",
      "epoch: 62, loss: 0.0318\n",
      "epoch: 63, loss: 0.0229\n",
      "epoch: 63, loss: 0.0316\n",
      "epoch: 64, loss: 0.0227\n",
      "epoch: 64, loss: 0.0314\n",
      "epoch: 65, loss: 0.0226\n",
      "epoch: 65, loss: 0.0312\n",
      "epoch: 66, loss: 0.0224\n",
      "epoch: 66, loss: 0.0310\n",
      "epoch: 67, loss: 0.0223\n",
      "epoch: 67, loss: 0.0308\n",
      "epoch: 68, loss: 0.0221\n",
      "epoch: 68, loss: 0.0306\n",
      "epoch: 69, loss: 0.0219\n",
      "epoch: 69, loss: 0.0304\n",
      "epoch: 70, loss: 0.0218\n",
      "epoch: 70, loss: 0.0302\n",
      "epoch: 71, loss: 0.0216\n",
      "epoch: 71, loss: 0.0300\n",
      "epoch: 72, loss: 0.0215\n",
      "epoch: 72, loss: 0.0298\n",
      "epoch: 73, loss: 0.0213\n",
      "epoch: 73, loss: 0.0296\n",
      "epoch: 74, loss: 0.0212\n",
      "epoch: 74, loss: 0.0294\n",
      "epoch: 75, loss: 0.0211\n",
      "epoch: 75, loss: 0.0293\n",
      "epoch: 76, loss: 0.0209\n",
      "epoch: 76, loss: 0.0291\n",
      "epoch: 77, loss: 0.0208\n",
      "epoch: 77, loss: 0.0289\n",
      "epoch: 78, loss: 0.0206\n",
      "epoch: 78, loss: 0.0287\n",
      "epoch: 79, loss: 0.0205\n",
      "epoch: 79, loss: 0.0285\n",
      "epoch: 80, loss: 0.0204\n",
      "epoch: 80, loss: 0.0284\n",
      "epoch: 81, loss: 0.0202\n",
      "epoch: 81, loss: 0.0282\n",
      "epoch: 82, loss: 0.0201\n",
      "epoch: 82, loss: 0.0280\n",
      "epoch: 83, loss: 0.0200\n",
      "epoch: 83, loss: 0.0279\n",
      "epoch: 84, loss: 0.0198\n",
      "epoch: 84, loss: 0.0277\n",
      "epoch: 85, loss: 0.0197\n",
      "epoch: 85, loss: 0.0275\n",
      "epoch: 86, loss: 0.0196\n",
      "epoch: 86, loss: 0.0274\n",
      "epoch: 87, loss: 0.0195\n",
      "epoch: 87, loss: 0.0272\n",
      "epoch: 88, loss: 0.0194\n",
      "epoch: 88, loss: 0.0270\n",
      "epoch: 89, loss: 0.0192\n",
      "epoch: 89, loss: 0.0269\n",
      "epoch: 90, loss: 0.0191\n",
      "epoch: 90, loss: 0.0267\n",
      "epoch: 91, loss: 0.0190\n",
      "epoch: 91, loss: 0.0266\n",
      "epoch: 92, loss: 0.0189\n",
      "epoch: 92, loss: 0.0264\n",
      "epoch: 93, loss: 0.0188\n",
      "epoch: 93, loss: 0.0263\n",
      "epoch: 94, loss: 0.0187\n",
      "epoch: 94, loss: 0.0261\n",
      "epoch: 95, loss: 0.0185\n",
      "epoch: 95, loss: 0.0260\n",
      "epoch: 96, loss: 0.0184\n",
      "epoch: 96, loss: 0.0258\n",
      "epoch: 97, loss: 0.0183\n",
      "epoch: 97, loss: 0.0257\n",
      "epoch: 98, loss: 0.0182\n",
      "epoch: 98, loss: 0.0255\n",
      "epoch: 99, loss: 0.0181\n",
      "epoch: 99, loss: 0.0254\n",
      "epoch: 100, loss: 0.0180\n",
      "epoch: 100, loss: 0.0253\n",
      "epoch: 101, loss: 0.0179\n",
      "epoch: 101, loss: 0.0251\n",
      "epoch: 102, loss: 0.0178\n",
      "epoch: 102, loss: 0.0250\n",
      "epoch: 103, loss: 0.0177\n",
      "epoch: 103, loss: 0.0249\n",
      "epoch: 104, loss: 0.0176\n",
      "epoch: 104, loss: 0.0247\n",
      "epoch: 105, loss: 0.0175\n",
      "epoch: 105, loss: 0.0246\n",
      "epoch: 106, loss: 0.0174\n",
      "epoch: 106, loss: 0.0245\n",
      "epoch: 107, loss: 0.0173\n",
      "epoch: 107, loss: 0.0243\n",
      "epoch: 108, loss: 0.0172\n",
      "epoch: 108, loss: 0.0242\n",
      "epoch: 109, loss: 0.0171\n",
      "epoch: 109, loss: 0.0241\n",
      "epoch: 110, loss: 0.0170\n",
      "epoch: 110, loss: 0.0239\n",
      "epoch: 111, loss: 0.0169\n",
      "epoch: 111, loss: 0.0238\n",
      "epoch: 112, loss: 0.0168\n",
      "epoch: 112, loss: 0.0237\n",
      "epoch: 113, loss: 0.0167\n",
      "epoch: 113, loss: 0.0236\n",
      "epoch: 114, loss: 0.0166\n",
      "epoch: 114, loss: 0.0234\n",
      "epoch: 115, loss: 0.0165\n",
      "epoch: 115, loss: 0.0233\n",
      "epoch: 116, loss: 0.0164\n",
      "epoch: 116, loss: 0.0232\n",
      "epoch: 117, loss: 0.0164\n",
      "epoch: 117, loss: 0.0231\n",
      "epoch: 118, loss: 0.0163\n",
      "epoch: 118, loss: 0.0230\n",
      "epoch: 119, loss: 0.0162\n",
      "epoch: 119, loss: 0.0229\n",
      "epoch: 120, loss: 0.0161\n",
      "epoch: 120, loss: 0.0227\n",
      "epoch: 121, loss: 0.0160\n",
      "epoch: 121, loss: 0.0226\n",
      "epoch: 122, loss: 0.0159\n",
      "epoch: 122, loss: 0.0225\n",
      "epoch: 123, loss: 0.0158\n",
      "epoch: 123, loss: 0.0224\n",
      "epoch: 124, loss: 0.0158\n",
      "epoch: 124, loss: 0.0223\n",
      "epoch: 125, loss: 0.0157\n",
      "epoch: 125, loss: 0.0222\n",
      "epoch: 126, loss: 0.0156\n",
      "epoch: 126, loss: 0.0221\n",
      "epoch: 127, loss: 0.0155\n",
      "epoch: 127, loss: 0.0220\n",
      "epoch: 128, loss: 0.0154\n",
      "epoch: 128, loss: 0.0219\n",
      "epoch: 129, loss: 0.0154\n",
      "epoch: 129, loss: 0.0218\n",
      "epoch: 130, loss: 0.0153\n",
      "epoch: 130, loss: 0.0216\n",
      "epoch: 131, loss: 0.0152\n",
      "epoch: 131, loss: 0.0215\n",
      "epoch: 132, loss: 0.0151\n",
      "epoch: 132, loss: 0.0214\n",
      "epoch: 133, loss: 0.0150\n",
      "epoch: 133, loss: 0.0213\n",
      "epoch: 134, loss: 0.0150\n",
      "epoch: 134, loss: 0.0212\n",
      "epoch: 135, loss: 0.0149\n",
      "epoch: 135, loss: 0.0211\n",
      "epoch: 136, loss: 0.0148\n",
      "epoch: 136, loss: 0.0210\n",
      "epoch: 137, loss: 0.0148\n",
      "epoch: 137, loss: 0.0209\n",
      "epoch: 138, loss: 0.0147\n",
      "epoch: 138, loss: 0.0208\n",
      "epoch: 139, loss: 0.0146\n",
      "epoch: 139, loss: 0.0207\n",
      "epoch: 140, loss: 0.0145\n",
      "epoch: 140, loss: 0.0206\n",
      "epoch: 141, loss: 0.0145\n",
      "epoch: 141, loss: 0.0206\n",
      "epoch: 142, loss: 0.0144\n",
      "epoch: 142, loss: 0.0205\n",
      "epoch: 143, loss: 0.0143\n",
      "epoch: 143, loss: 0.0204\n",
      "epoch: 144, loss: 0.0143\n",
      "epoch: 144, loss: 0.0203\n",
      "epoch: 145, loss: 0.0142\n",
      "epoch: 145, loss: 0.0202\n",
      "epoch: 146, loss: 0.0141\n",
      "epoch: 146, loss: 0.0201\n",
      "epoch: 147, loss: 0.0141\n",
      "epoch: 147, loss: 0.0200\n",
      "epoch: 148, loss: 0.0140\n",
      "epoch: 148, loss: 0.0199\n",
      "epoch: 149, loss: 0.0139\n",
      "epoch: 149, loss: 0.0198\n",
      "epoch: 150, loss: 0.0139\n",
      "epoch: 150, loss: 0.0197\n",
      "epoch: 151, loss: 0.0138\n",
      "epoch: 151, loss: 0.0196\n",
      "epoch: 152, loss: 0.0137\n",
      "epoch: 152, loss: 0.0196\n",
      "epoch: 153, loss: 0.0137\n",
      "epoch: 153, loss: 0.0195\n",
      "epoch: 154, loss: 0.0136\n",
      "epoch: 154, loss: 0.0194\n",
      "epoch: 155, loss: 0.0135\n",
      "epoch: 155, loss: 0.0193\n",
      "epoch: 156, loss: 0.0135\n",
      "epoch: 156, loss: 0.0192\n",
      "epoch: 157, loss: 0.0134\n",
      "epoch: 157, loss: 0.0191\n",
      "epoch: 158, loss: 0.0134\n",
      "epoch: 158, loss: 0.0191\n",
      "epoch: 159, loss: 0.0133\n",
      "epoch: 159, loss: 0.0190\n",
      "epoch: 160, loss: 0.0132\n",
      "epoch: 160, loss: 0.0189\n",
      "epoch: 161, loss: 0.0132\n",
      "epoch: 161, loss: 0.0188\n",
      "epoch: 162, loss: 0.0131\n",
      "epoch: 162, loss: 0.0187\n",
      "epoch: 163, loss: 0.0131\n",
      "epoch: 163, loss: 0.0187\n",
      "epoch: 164, loss: 0.0130\n",
      "epoch: 164, loss: 0.0186\n",
      "epoch: 165, loss: 0.0130\n",
      "epoch: 165, loss: 0.0185\n",
      "epoch: 166, loss: 0.0129\n",
      "epoch: 166, loss: 0.0184\n",
      "epoch: 167, loss: 0.0128\n",
      "epoch: 167, loss: 0.0183\n",
      "epoch: 168, loss: 0.0128\n",
      "epoch: 168, loss: 0.0183\n",
      "epoch: 169, loss: 0.0127\n",
      "epoch: 169, loss: 0.0182\n",
      "epoch: 170, loss: 0.0127\n",
      "epoch: 170, loss: 0.0181\n",
      "epoch: 171, loss: 0.0126\n",
      "epoch: 171, loss: 0.0180\n",
      "epoch: 172, loss: 0.0126\n",
      "epoch: 172, loss: 0.0180\n",
      "epoch: 173, loss: 0.0125\n",
      "epoch: 173, loss: 0.0179\n",
      "epoch: 174, loss: 0.0125\n",
      "epoch: 174, loss: 0.0178\n",
      "epoch: 175, loss: 0.0124\n",
      "epoch: 175, loss: 0.0177\n",
      "epoch: 176, loss: 0.0124\n",
      "epoch: 176, loss: 0.0177\n",
      "epoch: 177, loss: 0.0123\n",
      "epoch: 177, loss: 0.0176\n",
      "epoch: 178, loss: 0.0123\n",
      "epoch: 178, loss: 0.0175\n",
      "epoch: 179, loss: 0.0122\n",
      "epoch: 179, loss: 0.0175\n",
      "epoch: 180, loss: 0.0121\n",
      "epoch: 180, loss: 0.0174\n",
      "epoch: 181, loss: 0.0121\n",
      "epoch: 181, loss: 0.0173\n",
      "epoch: 182, loss: 0.0121\n",
      "epoch: 182, loss: 0.0173\n",
      "epoch: 183, loss: 0.0120\n",
      "epoch: 183, loss: 0.0172\n",
      "epoch: 184, loss: 0.0120\n",
      "epoch: 184, loss: 0.0171\n",
      "epoch: 185, loss: 0.0119\n",
      "epoch: 185, loss: 0.0171\n",
      "epoch: 186, loss: 0.0119\n",
      "epoch: 186, loss: 0.0170\n",
      "epoch: 187, loss: 0.0118\n",
      "epoch: 187, loss: 0.0169\n",
      "epoch: 188, loss: 0.0118\n",
      "epoch: 188, loss: 0.0169\n",
      "epoch: 189, loss: 0.0117\n",
      "epoch: 189, loss: 0.0168\n",
      "epoch: 190, loss: 0.0117\n",
      "epoch: 190, loss: 0.0167\n",
      "epoch: 191, loss: 0.0116\n",
      "epoch: 191, loss: 0.0167\n",
      "epoch: 192, loss: 0.0116\n",
      "epoch: 192, loss: 0.0166\n",
      "epoch: 193, loss: 0.0115\n",
      "epoch: 193, loss: 0.0165\n",
      "epoch: 194, loss: 0.0115\n",
      "epoch: 194, loss: 0.0165\n",
      "epoch: 195, loss: 0.0114\n",
      "epoch: 195, loss: 0.0164\n",
      "epoch: 196, loss: 0.0114\n",
      "epoch: 196, loss: 0.0164\n",
      "epoch: 197, loss: 0.0113\n",
      "epoch: 197, loss: 0.0163\n",
      "epoch: 198, loss: 0.0113\n",
      "epoch: 198, loss: 0.0162\n",
      "epoch: 199, loss: 0.0113\n",
      "epoch: 199, loss: 0.0162\n",
      "epoch: 200, loss: 0.0112\n",
      "epoch: 200, loss: 0.0161\n",
      "epoch: 201, loss: 0.0112\n",
      "epoch: 201, loss: 0.0160\n",
      "epoch: 202, loss: 0.0111\n",
      "epoch: 202, loss: 0.0160\n",
      "epoch: 203, loss: 0.0111\n",
      "epoch: 203, loss: 0.0159\n",
      "epoch: 204, loss: 0.0110\n",
      "epoch: 204, loss: 0.0159\n",
      "epoch: 205, loss: 0.0110\n",
      "epoch: 205, loss: 0.0158\n",
      "epoch: 206, loss: 0.0110\n",
      "epoch: 206, loss: 0.0158\n",
      "epoch: 207, loss: 0.0109\n",
      "epoch: 207, loss: 0.0157\n",
      "epoch: 208, loss: 0.0109\n",
      "epoch: 208, loss: 0.0156\n",
      "epoch: 209, loss: 0.0108\n",
      "epoch: 209, loss: 0.0156\n",
      "epoch: 210, loss: 0.0108\n",
      "epoch: 210, loss: 0.0155\n",
      "epoch: 211, loss: 0.0108\n",
      "epoch: 211, loss: 0.0155\n",
      "epoch: 212, loss: 0.0107\n",
      "epoch: 212, loss: 0.0154\n",
      "epoch: 213, loss: 0.0107\n",
      "epoch: 213, loss: 0.0154\n",
      "epoch: 214, loss: 0.0106\n",
      "epoch: 214, loss: 0.0153\n",
      "epoch: 215, loss: 0.0106\n",
      "epoch: 215, loss: 0.0153\n",
      "epoch: 216, loss: 0.0106\n",
      "epoch: 216, loss: 0.0152\n",
      "epoch: 217, loss: 0.0105\n",
      "epoch: 217, loss: 0.0151\n",
      "epoch: 218, loss: 0.0105\n",
      "epoch: 218, loss: 0.0151\n",
      "epoch: 219, loss: 0.0105\n",
      "epoch: 219, loss: 0.0150\n",
      "epoch: 220, loss: 0.0104\n",
      "epoch: 220, loss: 0.0150\n",
      "epoch: 221, loss: 0.0104\n",
      "epoch: 221, loss: 0.0149\n",
      "epoch: 222, loss: 0.0103\n",
      "epoch: 222, loss: 0.0149\n",
      "epoch: 223, loss: 0.0103\n",
      "epoch: 223, loss: 0.0148\n",
      "epoch: 224, loss: 0.0103\n",
      "epoch: 224, loss: 0.0148\n",
      "epoch: 225, loss: 0.0102\n",
      "epoch: 225, loss: 0.0147\n",
      "epoch: 226, loss: 0.0102\n",
      "epoch: 226, loss: 0.0147\n",
      "epoch: 227, loss: 0.0102\n",
      "epoch: 227, loss: 0.0146\n",
      "epoch: 228, loss: 0.0101\n",
      "epoch: 228, loss: 0.0146\n",
      "epoch: 229, loss: 0.0101\n",
      "epoch: 229, loss: 0.0145\n",
      "epoch: 230, loss: 0.0101\n",
      "epoch: 230, loss: 0.0145\n",
      "epoch: 231, loss: 0.0100\n",
      "epoch: 231, loss: 0.0144\n",
      "epoch: 232, loss: 0.0100\n",
      "epoch: 232, loss: 0.0144\n",
      "epoch: 233, loss: 0.0099\n",
      "epoch: 233, loss: 0.0143\n",
      "epoch: 234, loss: 0.0099\n",
      "epoch: 234, loss: 0.0143\n",
      "epoch: 235, loss: 0.0099\n",
      "epoch: 235, loss: 0.0142\n",
      "epoch: 236, loss: 0.0098\n",
      "epoch: 236, loss: 0.0142\n",
      "epoch: 237, loss: 0.0098\n",
      "epoch: 237, loss: 0.0141\n",
      "epoch: 238, loss: 0.0098\n",
      "epoch: 238, loss: 0.0141\n",
      "epoch: 239, loss: 0.0097\n",
      "epoch: 239, loss: 0.0141\n",
      "epoch: 240, loss: 0.0097\n",
      "epoch: 240, loss: 0.0140\n",
      "epoch: 241, loss: 0.0097\n",
      "epoch: 241, loss: 0.0140\n",
      "epoch: 242, loss: 0.0096\n",
      "epoch: 242, loss: 0.0139\n",
      "epoch: 243, loss: 0.0096\n",
      "epoch: 243, loss: 0.0139\n",
      "epoch: 244, loss: 0.0096\n",
      "epoch: 244, loss: 0.0138\n",
      "epoch: 245, loss: 0.0096\n",
      "epoch: 245, loss: 0.0138\n",
      "epoch: 246, loss: 0.0095\n",
      "epoch: 246, loss: 0.0137\n",
      "epoch: 247, loss: 0.0095\n",
      "epoch: 247, loss: 0.0137\n",
      "epoch: 248, loss: 0.0095\n",
      "epoch: 248, loss: 0.0136\n",
      "epoch: 249, loss: 0.0094\n",
      "epoch: 249, loss: 0.0136\n",
      "epoch: 250, loss: 0.0094\n",
      "epoch: 250, loss: 0.0136\n",
      "epoch: 251, loss: 0.0094\n",
      "epoch: 251, loss: 0.0135\n",
      "epoch: 252, loss: 0.0093\n",
      "epoch: 252, loss: 0.0135\n",
      "epoch: 253, loss: 0.0093\n",
      "epoch: 253, loss: 0.0134\n",
      "epoch: 254, loss: 0.0093\n",
      "epoch: 254, loss: 0.0134\n",
      "epoch: 255, loss: 0.0092\n",
      "epoch: 255, loss: 0.0133\n",
      "epoch: 256, loss: 0.0092\n",
      "epoch: 256, loss: 0.0133\n",
      "epoch: 257, loss: 0.0092\n",
      "epoch: 257, loss: 0.0133\n",
      "epoch: 258, loss: 0.0092\n",
      "epoch: 258, loss: 0.0132\n",
      "epoch: 259, loss: 0.0091\n",
      "epoch: 259, loss: 0.0132\n",
      "epoch: 260, loss: 0.0091\n",
      "epoch: 260, loss: 0.0131\n",
      "epoch: 261, loss: 0.0091\n",
      "epoch: 261, loss: 0.0131\n",
      "epoch: 262, loss: 0.0090\n",
      "epoch: 262, loss: 0.0131\n",
      "epoch: 263, loss: 0.0090\n",
      "epoch: 263, loss: 0.0130\n",
      "epoch: 264, loss: 0.0090\n",
      "epoch: 264, loss: 0.0130\n",
      "epoch: 265, loss: 0.0090\n",
      "epoch: 265, loss: 0.0129\n",
      "epoch: 266, loss: 0.0089\n",
      "epoch: 266, loss: 0.0129\n",
      "epoch: 267, loss: 0.0089\n",
      "epoch: 267, loss: 0.0129\n",
      "epoch: 268, loss: 0.0089\n",
      "epoch: 268, loss: 0.0128\n",
      "epoch: 269, loss: 0.0088\n",
      "epoch: 269, loss: 0.0128\n",
      "epoch: 270, loss: 0.0088\n",
      "epoch: 270, loss: 0.0127\n",
      "epoch: 271, loss: 0.0088\n",
      "epoch: 271, loss: 0.0127\n",
      "epoch: 272, loss: 0.0088\n",
      "epoch: 272, loss: 0.0127\n",
      "epoch: 273, loss: 0.0087\n",
      "epoch: 273, loss: 0.0126\n",
      "epoch: 274, loss: 0.0087\n",
      "epoch: 274, loss: 0.0126\n",
      "epoch: 275, loss: 0.0087\n",
      "epoch: 275, loss: 0.0126\n",
      "epoch: 276, loss: 0.0087\n",
      "epoch: 276, loss: 0.0125\n",
      "epoch: 277, loss: 0.0086\n",
      "epoch: 277, loss: 0.0125\n",
      "epoch: 278, loss: 0.0086\n",
      "epoch: 278, loss: 0.0124\n",
      "epoch: 279, loss: 0.0086\n",
      "epoch: 279, loss: 0.0124\n",
      "epoch: 280, loss: 0.0086\n",
      "epoch: 280, loss: 0.0124\n",
      "epoch: 281, loss: 0.0085\n",
      "epoch: 281, loss: 0.0123\n",
      "epoch: 282, loss: 0.0085\n",
      "epoch: 282, loss: 0.0123\n",
      "epoch: 283, loss: 0.0085\n",
      "epoch: 283, loss: 0.0123\n",
      "epoch: 284, loss: 0.0084\n",
      "epoch: 284, loss: 0.0122\n",
      "epoch: 285, loss: 0.0084\n",
      "epoch: 285, loss: 0.0122\n",
      "epoch: 286, loss: 0.0084\n",
      "epoch: 286, loss: 0.0122\n",
      "epoch: 287, loss: 0.0084\n",
      "epoch: 287, loss: 0.0121\n",
      "epoch: 288, loss: 0.0084\n",
      "epoch: 288, loss: 0.0121\n",
      "epoch: 289, loss: 0.0083\n",
      "epoch: 289, loss: 0.0121\n",
      "epoch: 290, loss: 0.0083\n",
      "epoch: 290, loss: 0.0120\n",
      "epoch: 291, loss: 0.0083\n",
      "epoch: 291, loss: 0.0120\n",
      "epoch: 292, loss: 0.0083\n",
      "epoch: 292, loss: 0.0119\n",
      "epoch: 293, loss: 0.0082\n",
      "epoch: 293, loss: 0.0119\n",
      "epoch: 294, loss: 0.0082\n",
      "epoch: 294, loss: 0.0119\n",
      "epoch: 295, loss: 0.0082\n",
      "epoch: 295, loss: 0.0118\n",
      "epoch: 296, loss: 0.0082\n",
      "epoch: 296, loss: 0.0118\n",
      "epoch: 297, loss: 0.0081\n",
      "epoch: 297, loss: 0.0118\n",
      "epoch: 298, loss: 0.0081\n",
      "epoch: 298, loss: 0.0117\n",
      "epoch: 299, loss: 0.0081\n",
      "epoch: 299, loss: 0.0117\n",
      "epoch: 300, loss: 0.0081\n",
      "epoch: 300, loss: 0.0117\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    for sentence, tags in training_data_clean:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Initialize hidden state of LSTM after each instance\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        # Turn inputs into tensors of word indices\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Forward pass\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Compute the loss, gradients and update parameters\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        print(f'epoch: {epoch+1}, loss: {loss:.4f}')\n",
    "        optimizer.step()"
   ]
  },
  {
   "source": [
    "Our model has now finished training. Let's print out some statistics to show how well the model training performed.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['the', 'dog', 'ate', 'the', 'apple'], ['everybody', 'read', 'that', 'book']]"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "training_sentences_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Before training:\n - initial probabilities: tensor([[0.2398, 0.2627, 0.4975],\n        [0.2419, 0.2779, 0.4802],\n        [0.2534, 0.2684, 0.4782],\n        [0.2489, 0.2651, 0.4860],\n        [0.2222, 0.2934, 0.4845]])\n - sentence: the dog ate the apple\n - predicition: ['Verb', 'Verb', 'Verb', 'Verb', 'Verb']\nAfter training:\n - final probabilities: tensor([[9.8002e-01, 1.1596e-02, 8.3884e-03],\n        [2.3367e-03, 9.9410e-01, 3.5672e-03],\n        [8.6107e-03, 6.2304e-03, 9.8516e-01],\n        [9.9327e-01, 8.0867e-04, 5.9249e-03],\n        [2.7279e-03, 9.9586e-01, 1.4118e-03]])\n - sentence: the dog ate the apple\n - prediction: ['Determiner', 'Noun', 'Verb', 'Determiner', 'Noun']\n\nBefore training:\n - initial probabilities: tensor([[0.2205, 0.3022, 0.4773],\n        [0.2305, 0.2911, 0.4784],\n        [0.2230, 0.2964, 0.4807],\n        [0.2240, 0.2953, 0.4807]])\n - sentence: everybody read that book\n - predicition: ['Verb', 'Verb', 'Verb', 'Verb']\nAfter training:\n - final probabilities: tensor([[1.1239e-03, 9.9522e-01, 3.6580e-03],\n        [1.5897e-02, 1.0782e-02, 9.7332e-01],\n        [9.8896e-01, 1.8482e-03, 9.1887e-03],\n        [2.4166e-03, 9.9687e-01, 7.0991e-04]])\n - sentence: everybody read that book\n - prediction: ['Noun', 'Verb', 'Determiner', 'Noun']\n\n"
     ]
    }
   ],
   "source": [
    "store_initial_predictions.reverse()\n",
    "store_initial_probabilities.reverse()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sentence in training_sentences_clean:\n",
    "        inputs = prepare_sequence(sentence, word_to_ix)\n",
    "        tag_scores = model(inputs)\n",
    "        tag_probabilities = tag_scores.exp()\n",
    "        max_values, max_indices = torch.max(tag_probabilities, 1)\n",
    "        predictions = [ix_to_tag[x] for x in max_indices.numpy()]\n",
    "\n",
    "        print('Before training:')\n",
    "        print(' - initial probabilities: {}'.format(store_initial_probabilities.pop()))\n",
    "        print(' - sentence: {}'.format(' '.join(sentence)))\n",
    "        print(' - predicition: {}'.format(store_initial_predictions.pop()))\n",
    "        print('After training:')\n",
    "        print(' - final probabilities: {}'.format(tag_probabilities))\n",
    "        print(' - sentence: {}'.format(' '.join(sentence)))\n",
    "        print(' - prediction: {}'.format(predictions))\n",
    "        print('')"
   ]
  },
  {
   "source": [
    "## Save Model\n",
    "Save the Pytorch model to disk. This model will be used in the deployment tutorial."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/Users/hzh/Desktop/Pytorch_Tutorial/models/model.pt'"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "models_path = os.path.join(os.getcwd(), 'models', 'model.pt')\n",
    "models_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), models_path)"
   ]
  },
  {
   "source": [
    "## Load Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(models_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LSTMTagger(\n",
       "  (word_embeddings): Embedding(8, 6)\n",
       "  (lstm): LSTM(6, 6)\n",
       "  (hidden2tag): Linear(in_features=6, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "source": [
    "Run a training example through the loaded model and make a prediction.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sentence: The dog ate the apple.\nparts-of-speach: ['Determiner', 'Noun', 'Verb', 'Determiner', 'Noun']\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_sentences_clean[0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "    tag_probabilities = tag_scores.exp()\n",
    "    max_values, max_indices = torch.max(tag_probabilities, 1)\n",
    "    predictions = [ix_to_tag[x] for x in max_indices.numpy()]\n",
    "    print('sentence: {}'.format(' '.join(training_sentences[0])))\n",
    "    print('parts-of-speach: {}'.format(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}